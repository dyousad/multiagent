#!/usr/bin/env python3
"""å…¨é¢éªŒè¯corpuså’Œç¼“å­˜çš„å®Œæ•´æ€§"""

import json
import pickle
from pathlib import Path

def analyze_corpus_completeness():
    """åˆ†æcorpuså’Œç¼“å­˜çš„å®Œæ•´æ€§"""
    print("="*80)
    print("HotpotQA Corpus & Cache å®Œæ•´æ€§åˆ†æ")
    print("="*80)
    print()

    # 1. åˆ†æï¿½ï¿½ï¿½å§‹æ•°æ®
    print("ğŸ“Š åŸå§‹æ•°æ®é›†åˆ†æ:")
    with open('data/hotpot_dev_fullwiki_v1.json') as f:
        hotpotqa_data = json.load(f)

    total_samples = len(hotpotqa_data)
    total_contexts = 0
    total_sentences = 0
    unique_titles = set()

    for sample in hotpotqa_data:
        context = sample.get('context', [])
        total_contexts += len(context)

        for title, sentences in context:
            unique_titles.add(title)
            if isinstance(sentences, list):
                total_sentences += len(sentences)
            else:
                total_sentences += 1

    print(f"   - HotpotQAæ ·æœ¬æ•°: {total_samples:,}")
    print(f"   - Contextå¯¹æ•°: {total_contexts:,}")
    print(f"   - å”¯ä¸€æ ‡é¢˜æ•°: {len(unique_titles):,}")
    print(f"   - é¢„è®¡å¥å­æ•°: {total_sentences:,}")

    # 2. åˆ†æcorpusæ–‡ä»¶
    print(f"\nğŸ“ Corpusæ–‡ä»¶åˆ†æ:")
    corpus_files = {
        'hotpotqa_corpus.json': 'data/hotpotqa_corpus.json',
        'hotpotqa_corpus_large.json': 'data/hotpotqa_corpus_large.json',
        'hotpotqa_corpus_full.json': 'data/hotpotqa_corpus_full.json'
    }

    for name, path in corpus_files.items():
        if Path(path).exists():
            with open(path) as f:
                data = json.load(f)
            size_mb = Path(path).stat().st_size / 1024 / 1024
            coverage = len(data) / total_sentences * 100
            print(f"   - {name}:")
            print(f"     è®°å½•æ•°: {len(data):,} ({coverage:.1f}% è¦†ç›–ç‡)")
            print(f"     æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")

    # 3. åˆ†æç¼“å­˜
    print(f"\nğŸ’¾ ç¼“å­˜æ–‡ä»¶åˆ†æ:")
    cache_dir = Path("data/cache")
    if cache_dir.exists():
        cache_files = list(cache_dir.glob("*.pkl"))
        print(f"   - ç¼“å­˜æ–‡ä»¶æ•°: {len(cache_files)}")

        for cache_file in cache_files:
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                size_mb = cache_file.stat().st_size / 1024 / 1024
                print(f"   - {cache_file.name}:")
                print(f"     è®°å½•æ•°: {len(data):,}")
                print(f"     æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
            except Exception as e:
                print(f"   - {cache_file.name}: è¯»å–å¤±è´¥ ({e})")

    # 4. å®Œæ•´æ€§æ£€æŸ¥
    print(f"\nğŸ” å®Œæ•´æ€§æ£€æŸ¥:")

    # æ£€æŸ¥æ˜¯å¦åŸºäºå®Œæ•´æ•°æ®é›†
    full_corpus_path = "data/hotpotqa_corpus_full.json"
    if Path(full_corpus_path).exists():
        with open(full_corpus_path) as f:
            full_corpus = json.load(f)

        print(f"   - å®Œæ•´corpusè®°å½•æ•°: {len(full_corpus):,}")
        print(f"   - è¦†ç›–ç‡: {len(full_corpus)/total_sentences*100:.1f}%")

        if len(full_corpus) >= total_sentences * 0.85:  # è€ƒè™‘å»é‡å’Œè¿‡æ»¤
            print(f"   âœ… è¦†ç›–ç‡è‰¯å¥½ (â‰¥85%)")
        else:
            print(f"   âš ï¸  è¦†ç›–ç‡å¯èƒ½ä¸è¶³")

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åŸºäºå®Œæ•´corpus
        corpus_cache = "data/cache/hotpotqa_corpus_full_BAAI_bge-large-en-v1.5_corpus.pkl"
        texts_cache = "data/cache/hotpotqa_corpus_full_BAAI_bge-large-en-v1.5_texts.pkl"

        if Path(corpus_cache).exists() and Path(texts_cache).exists():
            with open(corpus_cache, 'rb') as f:
                cached_corpus = pickle.load(f)
            with open(texts_cache, 'rb') as f:
                cached_texts = pickle.load(f)

            print(f"   - ç¼“å­˜corpusè®°å½•æ•°: {len(cached_corpus):,}")
            print(f"   - ç¼“å­˜textsè®°å½•æ•°: {len(cached_texts):,}")

            if len(cached_corpus) == len(full_corpus):
                print(f"   âœ… ç¼“å­˜ä¸å®Œæ•´corpusä¸€è‡´")
            else:
                print(f"   âš ï¸  ç¼“å­˜ä¸corpuså¤§å°ä¸åŒ¹é…")
                print(f"      å·®å¼‚: {abs(len(cached_corpus) - len(full_corpus)):,}æ¡è®°å½•")

    # 5. æ½œåœ¨é—®é¢˜æ£€æŸ¥
    print(f"\nğŸš¨ æ½œåœ¨é—®é¢˜æ£€æŸ¥:")

    issues = []

    # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å…¶ä»–æ•°æ®æº
    if total_sentences - len(full_corpus) > total_sentences * 0.15:
        issues.append("å¯èƒ½ç¼ºå°‘å¤§é‡å¥å­æ•°æ®")

    # æ£€æŸ¥ç¼“å­˜æ¨¡å‹
    bge_cache_exists = any("bge-large-en-v1.5" in str(f) for f in cache_dir.glob("*.pkl"))
    if not bge_cache_exists:
        issues.append("ç¼ºå°‘BGE embeddingæ¨¡å‹çš„ç¼“å­˜")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªcorpusç‰ˆæœ¬
    small_corpus_exists = Path("data/hotpotqa_corpus.json").exists()
    if small_corpus_exists:
        issues.append("å­˜åœ¨å¤šä¸ªcorpusç‰ˆæœ¬ï¼Œå¯èƒ½å¯¼è‡´æ··æ·†")

    if not issues:
        print(f"   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    else:
        for issue in issues:
            print(f"   âš ï¸  {issue}")

    # 6. å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"   1. ä½¿ç”¨ 'hotpotqa_corpus_full.json' ä½œä¸ºä¸»è¦corpus")
    print(f"   2. ç¡®ä¿RAGç³»ç»ŸæŒ‡å‘å®Œæ•´ç¼“å­˜æ–‡ä»¶")
    print(f"   3. è€ƒè™‘åˆ é™¤è¾ƒå°çš„corpusæ–‡ä»¶é¿å…æ··æ·†")
    print(f"   4. ç¼“å­˜å·²åŸºäºå®Œæ•´æ•°æ®é›†ç”Ÿæˆï¼Œå¯ç›´æ¥ä½¿ç”¨")

    print(f"\n" + "="*80)
    print(f"âœ… åˆ†æå®Œæˆ")
    print(f"="*80)

if __name__ == "__main__":
    analyze_corpus_completeness()