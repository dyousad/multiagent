#!/usr/bin/env python3
"""éªŒè¯åµŒå…¥ç¼“å­˜çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§"""

import sys
from pathlib import Path
import pickle
import numpy as np

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def verify_embedding_cache():
    """éªŒè¯åµŒå…¥ç¼“å­˜çš„å®Œæ•´æ€§"""
    print("ğŸ” éªŒè¯åµŒå…¥ç¼“å­˜å®Œæ•´æ€§")
    print("="*60)

    cache_dir = Path("data/cache")
    model_name = "BAAI_bge-large-en-v1.5"

    # é¢„æœŸçš„ç¼“å­˜æ–‡ä»¶
    expected_files = {
        'corpus': f"hotpotqa_corpus_full_{model_name}_corpus.pkl",
        'texts': f"hotpotqa_corpus_full_{model_name}_texts.pkl",
        'embeddings': f"hotpotqa_corpus_full_{model_name}_embeddings.npy",
        'index': f"hotpotqa_corpus_full_{model_name}_index.faiss"
    }

    print("ğŸ“ æ£€æŸ¥ç¼“å­˜æ–‡ä»¶:")
    existing_files = {}
    for key, filename in expected_files.items():
        filepath = cache_dir / filename
        if filepath.exists():
            size_mb = filepath.stat().st_size / 1024 / 1024
            existing_files[key] = filepath
            print(f"   âœ… {filename} ({size_mb:.1f} MB)")
        else:
            print(f"   âŒ {filename} (ç¼ºå¤±)")

    if not existing_files:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç¼“å­˜æ–‡ä»¶ï¼")
        return

    print(f"\nğŸ“Š ç¼“å­˜å†…å®¹éªŒè¯:")

    # éªŒè¯corpusç¼“å­˜
    if 'corpus' in existing_files:
        try:
            with open(existing_files['corpus'], 'rb') as f:
                corpus = pickle.load(f)
            print(f"   Corpus: {len(corpus):,} æ¡è®°å½•")

            # æ£€æŸ¥corpusç»“æ„
            if corpus and isinstance(corpus[0], dict):
                sample_doc = corpus[0]
                keys = list(sample_doc.keys())
                print(f"   Corpusç»“æ„: {keys}")
            else:
                print("   âš ï¸  Corpusæ ¼å¼å¼‚å¸¸")
        except Exception as e:
            print(f"   âŒ Corpusè¯»å–å¤±è´¥: {e}")

    # éªŒè¯textsç¼“å­˜
    if 'texts' in existing_files:
        try:
            with open(existing_files['texts'], 'rb') as f:
                texts = pickle.load(f)
            print(f"   Texts: {len(texts):,} æ¡è®°å½•")

            if texts:
                avg_length = sum(len(text) for text in texts[:1000]) / min(1000, len(texts))
                print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        except Exception as e:
            print(f"   âŒ Textsè¯»å–å¤±è´¥: {e}")

    # éªŒè¯embeddings
    if 'embeddings' in existing_files:
        try:
            embeddings = np.load(existing_files['embeddings'])
            print(f"   Embeddings: {embeddings.shape}")
            print(f"   ç»´åº¦: {embeddings.shape[1]}")
            print(f"   æ•°æ®ç±»å‹: {embeddings.dtype}")
        except Exception as e:
            print(f"   âŒ Embeddingsè¯»å–å¤±è´¥: {e}")

    # éªŒè¯FAISSç´¢å¼•
    if 'index' in existing_files:
        try:
            import faiss
            index = faiss.read_index(str(existing_files['index']))
            print(f"   FAISSï¿½ï¿½å¼•: {index.ntotal} å‘é‡")
            print(f"   ç´¢å¼•ç»´åº¦: {index.d}")
            print(f"   ç´¢å¼•ç±»å‹: {type(index).__name__}")
        except ImportError:
            print(f"   âš ï¸  FAISSæœªå®‰è£…ï¼Œæ— æ³•éªŒè¯ç´¢å¼•")
        except Exception as e:
            print(f"   âŒ FAISSç´¢å¼•è¯»å–å¤±è´¥: {e}")

    # ä¸€è‡´æ€§æ£€æŸ¥
    print(f"\nğŸ” ä¸€è‡´æ€§æ£€æŸ¥:")
    counts = {}

    if 'corpus' in existing_files:
        counts['corpus'] = len(corpus)
    if 'texts' in existing_files:
        counts['texts'] = len(texts)
    if 'embeddings' in existing_files:
        counts['embeddings'] = embeddings.shape[0]
    if 'index' in existing_files and 'index' in locals():
        counts['index'] = index.ntotal

    if len(set(counts.values())) == 1:
        print(f"   âœ… æ‰€æœ‰ç¼“å­˜æ–‡ä»¶æ•°é‡ä¸€è‡´: {list(counts.values())[0]:,}")
    else:
        print(f"   âš ï¸  ç¼“å­˜æ–‡ä»¶æ•°é‡ä¸ä¸€è‡´:")
        for key, count in counts.items():
            print(f"      {key}: {count:,}")

    # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    print(f"\nğŸ§ª åŠŸèƒ½æµ‹è¯•:")
    try:
        from cached_retrieval_manager import CachedRetrievalManager

        print("   åˆå§‹åŒ–CachedRetrievalManager...")
        manager = CachedRetrievalManager(
            corpus_path="data/hotpotqa_corpus_full.json",
            model_name="BAAI/bge-large-en-v1.5",
            cache_dir="data/cache"
        )

        print(f"   ManageråŠ è½½æˆåŠŸ: {len(manager.corpus):,} æ–‡æ¡£")

        # æµ‹è¯•æ£€ç´¢
        test_query = "Scott Derrickson nationality"
        results = manager.retrieve(test_query, top_k=3)

        print(f"   æµ‹è¯•æ£€ç´¢: '{test_query}'")
        print(f"   è¿”å›ç»“æœ: {len(results)} æ¡")
        if results:
            print(f"   é¦–ä¸ªç»“æœ: {results[0][:100]}...")
            print("   âœ… æ£€ç´¢åŠŸèƒ½æ­£å¸¸")
        else:
            print("   âš ï¸  æ£€ç´¢æ— ç»“æœ")

    except Exception as e:
        print(f"   âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")

    print(f"\n" + "="*60)

if __name__ == "__main__":
    verify_embedding_cache()